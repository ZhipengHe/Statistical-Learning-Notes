{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Statistical Learning NOTE : This website is still under the construction. This website is aimed at excerpting some fundamental knowledge in statistical learning for self-learning. The main structure of this website is from the table of contents in the Statistical Learning textbook An Introduction to Statistical Learning . References [1] An Introduction to Statistical Learning: with Applications in R, Second Edition [2] Data Science and Machine Learning: Mathematical and Statistical Methods, First Edition","title":"Home"},{"location":"#statistical-learning","text":"NOTE : This website is still under the construction. This website is aimed at excerpting some fundamental knowledge in statistical learning for self-learning. The main structure of this website is from the table of contents in the Statistical Learning textbook An Introduction to Statistical Learning .","title":"Statistical Learning"},{"location":"#references","text":"[1] An Introduction to Statistical Learning: with Applications in R, Second Edition [2] Data Science and Machine Learning: Mathematical and Statistical Methods, First Edition","title":"References"},{"location":"about/","text":"Desierant in dilectaque totiens mortali templa herbosaque Annos erat hi satis Lorem markdownum Paphius nuper conduntur, aere habent acies: patent. Vertere Gradivus exercent alta fore detinuit Aphidas petere ab mecum per possit, festa. Sine flectant Hippodamas duros securi, et cum Dixit superi germana herbis, ora tenens, fas fila iterum ipsa. Simul naufraga nec domibus genuumque recidendum vir motamque notam ferant , silvis, signis. Lecto sequuntur vaga, caelesti quotiens , ore! Avis potum motu geminis suam! Plebi disque tabellae arescere procorum quis pignora Petit exemplumque flentem laude feritatis, bello Megareius puellis petit iubenti ferrugine ? Victorem et, locus haec aera de recedit virgo verborum tamque. Tam arcus silentum, at viae quoque , in cruentae prensoque primo praecincti tempus? Et nebulae Et omnes pennatis virentes ad damnum nigraque Ostendere iacto tenues Referunt lapidis inducta indomitae bacchae tutae faciem Ultima opto summa grando cornu inmemor rogis revirescere opus, inane, magni vota aniles non vocat? Spectant me lecti pectine ad annos, non suum es. Viscera fidem moenibus dedit caede latent clipeata nonne qui. Adfectu pius sera dixit, insistere, esse ex coronat cornua. Coniunx nigrae at montibus viri altera haberet Amplexuque virum redimitus fama quae sensit nitens Cerberus cum admoto locum orbes Telephus muneris labor ulterius atque. Mordetur conspicuus ne extulit potest, pallentemque lenius proles tendite tempore quamquam oraque. Iniuria nascitur id natam minoris animum. Saxo oppositas: adsueto sine, et fatis viscere edidit, ac plura ventis vale, bis miles. Eburnas tollit mons quo enim iungi , ad ebur. Erat texta, manibus, dis nervo nactus crura quidem ac lacrimis in. Aper hoc sine hoc precor, deae petit obest orbes si huic Phrygios tot nomen, repetit mei! Ferrumque si furit Carmina fiuntque nec quae turba, secundum? method_cable += jpegPlatform; ofMediaOffice *= thyristorTerminal; var pad_switch_web = servlet_cycle_log - cybersquatter_systray_node.management_mask_input(-5) / asp_serial_mnemonic * impactWww - virus_surge; meta_kbps.xp_wiki(pageNetmaskTft.ip_artificial(registryWebsite, flaming_web, tapeVaporwareSource) * minisite, aluRfid(alpha_shell_device, 37, webPackDigital), font); if (file_dock - jfsCps.thermistor(vertical_web, 3, superscalar_menu)) { flash(spam_leopard_dual, bookmark_encoding_laptop); web_broadband_queue.snmpNvram += key; } else { mbps_icio_camera += extranetIsa(scsiPointInternic( wysiwygManagementRoom)); encryption *= uriFlowchartReadme - vpnToolbarFi; systray_gpu_bloatware += primary(host) + smart(pppOn, data_cps_property); } Victa reperire? Quae vivere horum fert ligavit nunc fores, sermonibus servat iam ! Iacent uti cecinisse, ea dolori nunc quo fuso tempus possis et septem inrita madent; nos! Modo pascere neve, inarata pecorumque, qua se et, commissa in tectus Herculeos rebus ille cineres .","title":"About"},{"location":"about/#desierant-in-dilectaque-totiens-mortali-templa-herbosaque","text":"","title":"Desierant in dilectaque totiens mortali templa herbosaque"},{"location":"about/#annos-erat-hi-satis","text":"Lorem markdownum Paphius nuper conduntur, aere habent acies: patent. Vertere Gradivus exercent alta fore detinuit Aphidas petere ab mecum per possit, festa. Sine flectant Hippodamas duros securi, et cum Dixit superi germana herbis, ora tenens, fas fila iterum ipsa. Simul naufraga nec domibus genuumque recidendum vir motamque notam ferant , silvis, signis. Lecto sequuntur vaga, caelesti quotiens , ore! Avis potum motu geminis suam!","title":"Annos erat hi satis"},{"location":"about/#plebi-disque-tabellae-arescere-procorum-quis-pignora","text":"Petit exemplumque flentem laude feritatis, bello Megareius puellis petit iubenti ferrugine ? Victorem et, locus haec aera de recedit virgo verborum tamque. Tam arcus silentum, at viae quoque , in cruentae prensoque primo praecincti tempus? Et nebulae Et omnes pennatis virentes ad damnum nigraque Ostendere iacto tenues Referunt lapidis inducta indomitae bacchae tutae faciem Ultima opto summa grando cornu inmemor rogis revirescere opus, inane, magni vota aniles non vocat? Spectant me lecti pectine ad annos, non suum es. Viscera fidem moenibus dedit caede latent clipeata nonne qui. Adfectu pius sera dixit, insistere, esse ex coronat cornua.","title":"Plebi disque tabellae arescere procorum quis pignora"},{"location":"about/#coniunx-nigrae-at-montibus-viri-altera-haberet","text":"Amplexuque virum redimitus fama quae sensit nitens Cerberus cum admoto locum orbes Telephus muneris labor ulterius atque. Mordetur conspicuus ne extulit potest, pallentemque lenius proles tendite tempore quamquam oraque. Iniuria nascitur id natam minoris animum. Saxo oppositas: adsueto sine, et fatis viscere edidit, ac plura ventis vale, bis miles. Eburnas tollit mons quo enim iungi , ad ebur. Erat texta, manibus, dis nervo nactus crura quidem ac lacrimis in. Aper hoc sine hoc precor, deae petit obest orbes si huic Phrygios tot nomen, repetit mei! Ferrumque si furit Carmina fiuntque nec quae turba, secundum? method_cable += jpegPlatform; ofMediaOffice *= thyristorTerminal; var pad_switch_web = servlet_cycle_log - cybersquatter_systray_node.management_mask_input(-5) / asp_serial_mnemonic * impactWww - virus_surge; meta_kbps.xp_wiki(pageNetmaskTft.ip_artificial(registryWebsite, flaming_web, tapeVaporwareSource) * minisite, aluRfid(alpha_shell_device, 37, webPackDigital), font); if (file_dock - jfsCps.thermistor(vertical_web, 3, superscalar_menu)) { flash(spam_leopard_dual, bookmark_encoding_laptop); web_broadband_queue.snmpNvram += key; } else { mbps_icio_camera += extranetIsa(scsiPointInternic( wysiwygManagementRoom)); encryption *= uriFlowchartReadme - vpnToolbarFi; systray_gpu_bloatware += primary(host) + smart(pppOn, data_cps_property); } Victa reperire? Quae vivere horum fert ligavit nunc fores, sermonibus servat iam ! Iacent uti cecinisse, ea dolori nunc quo fuso tempus possis et septem inrita madent; nos! Modo pascere neve, inarata pecorumque, qua se et, commissa in tectus Herculeos rebus ille cineres .","title":"Coniunx nigrae at montibus viri altera haberet"},{"location":"ch2_notes/","text":"Chapter 2: Statistical Learning 2.1 What is statistical learning? Statistical learning refers to a set of approaches for estimating \\(f\\) . \\[Y=f(X)+\\epsilon\\] Here \\(f\\) is some fixed but unknown function of different predictors , \\(X_1,X_2, \\ldots, X_p,\\) and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about response \\(Y\\) . 2.1.1 Why Estimate \\(f\\) ? \u27a4 Prediction A set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. We can predict \\(Y\\) using \\[\\hat Y=\\hat f(X)\\] where \\(\\hat f\\) represents our estimate for f, and \\(\\hat Y\\) represents the resulting prediction for Y. In this setting, \\(\\hat f\\) is often treated as a black box , in the sense that one is not typically concerned with the exact form of \\(\\hat f\\) , provided that it yields accurate predictions for \\(Y\\) . The accuracy of \\(\\hat Y\\) as a prediction for \\(Y\\) depends on two quantities, which we will call the reducible error and the irreducible error . The reducible error is reducible because we can potentially improve the accuracy of \\(\\hat Y\\) by using the most appropriate statistical learning technique to estimate \\(f\\) . For the irreducible error, even if it were possible to form a perfect estimate for \\(f\\) , so that our estimated response took the form \\(\\hat Y = f(X)\\) , our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\) , which, by definition, cannot be predicted using \\(X\\) . Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f\\) , we cannot reduce the error introduced by \\(\\epsilon\\) . Consider a given estimate \\(\\hat Y\\) and a set of predictors \\(X\\) , which yields the prediction \\(\\hat Y=\\hat f(X)\\) . Assume for a moment that both \\(\\hat f\\) and \\(X\\) are fixed, so that the only variability comes from \\(\\epsilon\\) . Then, it is easy to show that \\[E(Y- \\hat Y)^2 = E[f(X)+\\epsilon- \\hat f(X)]^2 = [f(X)- \\hat f(X)]^2+ Var(\\epsilon),\\] where \\(E(Y- \\hat Y)^2\\) represents the average, or expected value , of the squared difference between the predicted and actual value of \\(Y\\) , and \\(Var(\\epsilon)\\) represents the variance associated with the error term \\(\\epsilon\\) . \u27a4 Inference Different from prediction, inference is aimed at understanding the association between \\(Y\\) and \\(X_1, \\ldots, X_p\\) . In this situation we wish to estimate \\(f\\) , but our goal is not necessarily to make predictions for \\(Y\\) . Now \\(\\hat f\\) cannot be treated as a black box, because we need to know its exact form. \u27a4 Prediction & Inference Some modeling could be conducted both for prediction and inference. Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating \\(f\\) may be appropriate. 2.1.2 How Do We Estimate \\(f\\) ? \u27a4 Parametric Methods Parametric methods involve a two-step model-based approach. First, we make an assumption about the functional form, or shape, of \\(f\\) . For example, one very simple assumption is that \\(f\\) is linear in \\(X\\) : \\[f(X)=\\beta_0 +\\beta_1 X_1+\\beta_2 X-2 +\\ldots+\\beta_p X_p\\] Once we have assumed that f is linear, the problem of estimating \\(f\\) is greatly simplified. Instead of having to estimate an entirely arbitrary \\(p\\) -dimensional function \\(f(X)\\) , one only needs to estimate the \\(p + 1\\) coefficients \\(\\beta_1+\\beta_2+\\ldots+\\beta_p\\) . After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters \\(\\beta_1+\\beta_2+\\ldots+\\beta_p\\) . That is, we want to find values of these parameters such that \\[Y \\approx \\beta_0 +\\beta_1 X_1+\\beta_2 X-2 +\\ldots+\\beta_p X_p\\] The model-based approach just described is referred to as parametric ; it reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters. Advantages : Simpler : Assuming a parametric form for \\(f\\) simplifies the problem of estimating \\(f\\) because it is generally much easier to estimate a set of parameters than to fit an entirely arbitrary function \\(f\\) . Speed : Parametric models are very fast to learn from data. Less Data : They do not require as much training data and can work well even if the fit to the data is not perfect. Disadvantages : Constrained : By choosing a functional form these methods are highly constrained to the specified form. Limited Complexity : The methods are more suited to simpler problems. Poor Fit : The chosen model will usually not match the true unknown form of \\(f\\) . Examples : Linear Regression Logistic Regression Perceptron Naive Bayes Simple Neural Networks \u27a4 Non-Parametric Methods Non-parametric methods do not make explicit assumptions about the functional form of \\(f\\) . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. Advantages : Flexibility : Capable of fitting a large number of functional forms. Power : No assumptions (or weak assumptions) about the underlying function. Performance : Can result in higher performance models for prediction. Disadvantages : More data : Require a lot more training data to estimate the mapping function. Slower : A lot slower to train as they often have far more parameters to train. Overfitting : More of a risk to overfit the training data and it is harder to explain why specific predictions are made. Examples : k-Nearest Neighbors Support Vector Machine Random Forests 2.1.3 Prediction Accuracy and Model Interpretability 2.1.4 Supervised Versus Unsupervised Learning 2.1.5 Regression Versus Classification Problems Reference [1] An Introduction to Statistical Learning: with Applications in R, Second Edition , Chapter 2 [2] Parametric and Nonparametric Machine Learning Algorithms , Blog Post","title":"Chapter 2: Statistical Learning"},{"location":"ch2_notes/#chapter-2-statistical-learning","text":"","title":"Chapter 2: Statistical Learning"},{"location":"ch2_notes/#21-what-is-statistical-learning","text":"Statistical learning refers to a set of approaches for estimating \\(f\\) . \\[Y=f(X)+\\epsilon\\] Here \\(f\\) is some fixed but unknown function of different predictors , \\(X_1,X_2, \\ldots, X_p,\\) and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about response \\(Y\\) .","title":"2.1 What is statistical learning?"},{"location":"ch2_notes/#211-why-estimate-f","text":"","title":"2.1.1 Why Estimate \\(f\\)?"},{"location":"ch2_notes/#prediction","text":"A set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. We can predict \\(Y\\) using \\[\\hat Y=\\hat f(X)\\] where \\(\\hat f\\) represents our estimate for f, and \\(\\hat Y\\) represents the resulting prediction for Y. In this setting, \\(\\hat f\\) is often treated as a black box , in the sense that one is not typically concerned with the exact form of \\(\\hat f\\) , provided that it yields accurate predictions for \\(Y\\) . The accuracy of \\(\\hat Y\\) as a prediction for \\(Y\\) depends on two quantities, which we will call the reducible error and the irreducible error . The reducible error is reducible because we can potentially improve the accuracy of \\(\\hat Y\\) by using the most appropriate statistical learning technique to estimate \\(f\\) . For the irreducible error, even if it were possible to form a perfect estimate for \\(f\\) , so that our estimated response took the form \\(\\hat Y = f(X)\\) , our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\) , which, by definition, cannot be predicted using \\(X\\) . Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f\\) , we cannot reduce the error introduced by \\(\\epsilon\\) . Consider a given estimate \\(\\hat Y\\) and a set of predictors \\(X\\) , which yields the prediction \\(\\hat Y=\\hat f(X)\\) . Assume for a moment that both \\(\\hat f\\) and \\(X\\) are fixed, so that the only variability comes from \\(\\epsilon\\) . Then, it is easy to show that \\[E(Y- \\hat Y)^2 = E[f(X)+\\epsilon- \\hat f(X)]^2 = [f(X)- \\hat f(X)]^2+ Var(\\epsilon),\\] where \\(E(Y- \\hat Y)^2\\) represents the average, or expected value , of the squared difference between the predicted and actual value of \\(Y\\) , and \\(Var(\\epsilon)\\) represents the variance associated with the error term \\(\\epsilon\\) .","title":"&#10148; Prediction"},{"location":"ch2_notes/#inference","text":"Different from prediction, inference is aimed at understanding the association between \\(Y\\) and \\(X_1, \\ldots, X_p\\) . In this situation we wish to estimate \\(f\\) , but our goal is not necessarily to make predictions for \\(Y\\) . Now \\(\\hat f\\) cannot be treated as a black box, because we need to know its exact form.","title":"&#10148; Inference"},{"location":"ch2_notes/#prediction-inference","text":"Some modeling could be conducted both for prediction and inference. Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating \\(f\\) may be appropriate.","title":"&#10148; Prediction &amp; Inference"},{"location":"ch2_notes/#212-how-do-we-estimate-f","text":"","title":"2.1.2 How Do We Estimate \\(f\\)?"},{"location":"ch2_notes/#parametric-methods","text":"Parametric methods involve a two-step model-based approach. First, we make an assumption about the functional form, or shape, of \\(f\\) . For example, one very simple assumption is that \\(f\\) is linear in \\(X\\) : \\[f(X)=\\beta_0 +\\beta_1 X_1+\\beta_2 X-2 +\\ldots+\\beta_p X_p\\] Once we have assumed that f is linear, the problem of estimating \\(f\\) is greatly simplified. Instead of having to estimate an entirely arbitrary \\(p\\) -dimensional function \\(f(X)\\) , one only needs to estimate the \\(p + 1\\) coefficients \\(\\beta_1+\\beta_2+\\ldots+\\beta_p\\) . After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters \\(\\beta_1+\\beta_2+\\ldots+\\beta_p\\) . That is, we want to find values of these parameters such that \\[Y \\approx \\beta_0 +\\beta_1 X_1+\\beta_2 X-2 +\\ldots+\\beta_p X_p\\] The model-based approach just described is referred to as parametric ; it reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters. Advantages : Simpler : Assuming a parametric form for \\(f\\) simplifies the problem of estimating \\(f\\) because it is generally much easier to estimate a set of parameters than to fit an entirely arbitrary function \\(f\\) . Speed : Parametric models are very fast to learn from data. Less Data : They do not require as much training data and can work well even if the fit to the data is not perfect. Disadvantages : Constrained : By choosing a functional form these methods are highly constrained to the specified form. Limited Complexity : The methods are more suited to simpler problems. Poor Fit : The chosen model will usually not match the true unknown form of \\(f\\) . Examples : Linear Regression Logistic Regression Perceptron Naive Bayes Simple Neural Networks","title":"&#10148; Parametric Methods"},{"location":"ch2_notes/#non-parametric-methods","text":"Non-parametric methods do not make explicit assumptions about the functional form of \\(f\\) . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. Advantages : Flexibility : Capable of fitting a large number of functional forms. Power : No assumptions (or weak assumptions) about the underlying function. Performance : Can result in higher performance models for prediction. Disadvantages : More data : Require a lot more training data to estimate the mapping function. Slower : A lot slower to train as they often have far more parameters to train. Overfitting : More of a risk to overfit the training data and it is harder to explain why specific predictions are made. Examples : k-Nearest Neighbors Support Vector Machine Random Forests","title":"&#10148; Non-Parametric Methods"},{"location":"ch2_notes/#213-prediction-accuracy-and-model-interpretability","text":"","title":"2.1.3 Prediction Accuracy and Model Interpretability"},{"location":"ch2_notes/#214-supervised-versus-unsupervised-learning","text":"","title":"2.1.4 Supervised Versus Unsupervised Learning"},{"location":"ch2_notes/#215-regression-versus-classification-problems","text":"","title":"2.1.5 Regression Versus Classification Problems"},{"location":"ch2_notes/#reference","text":"[1] An Introduction to Statistical Learning: with Applications in R, Second Edition , Chapter 2 [2] Parametric and Nonparametric Machine Learning Algorithms , Blog Post","title":"Reference"}]}